{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6b2DivcIf36VldLDZKTMQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrpahadi2609/Statistics/blob/main/Statistics_Advance_1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvhHcwZC-kvv"
      },
      "outputs": [],
      "source": [
        "## QUES 1) Explain the properties of the F-distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is a continuous probability distribution that arises frequently in statistics, particularly in the context of variance analysis and hypothesis testing. Here are the key properties of the F-distribution:\n",
        "\n",
        "1. Definition: The F-distribution is defined as the ratio of two scaled chi-squared distributions. Specifically, if \\(X\\) and \\(Y\\) are independent chi-squared random variables with \\(d_1\\) and \\(d_2\\) degrees of freedom, respectively, then the random variable \\(F\\) defined as:\n",
        "   \\[\n",
        "   F = \\frac{(X/d_1)}{(Y/d_2)}\n",
        "   \\]\n",
        "   follows an F-distribution with \\(d_1\\) and \\(d_2\\) degrees of freedom.\n",
        "\n",
        "2. Degrees of Freedom: The F-distribution is characterized by two sets of degrees of freedom:\n",
        "   - \\(d_1\\) (numerator degrees of freedom)\n",
        "   - \\(d_2\\) (denominator degrees of freedom)\n",
        "\n",
        "3. Shape: The F-distribution is positively skewed, meaning it has a long right tail. The shape of the distribution changes based on the degrees of freedom:\n",
        "   - As \\(d_1\\) and \\(d_2\\) increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
        "\n",
        "4. Range: The F-distribution only takes positive values, ranging from \\(0\\) to \\(+\\infty\\).\n",
        "\n",
        "5. Mean and Variance:\n",
        "   - The mean of the F-distribution is given by:\n",
        "     \\[\n",
        "     \\text{Mean} = \\frac{d_2}{d_2 - 2} \\quad \\text{(for } d_2 > 2\\text{)}\n",
        "     \\]\n",
        "   - The variance is given by:\n",
        "     \\[\n",
        "     \\text{Variance} = \\frac{2(d_2^2)(d_1 + d_1 - 2)}{d_1(d_2 - 2)^2(d_2 - 4)} \\quad \\text{(for } d_2 > 4\\text{)}\n",
        "     \\]\n",
        "\n",
        "6. Applications: The F-distribution is commonly used in:\n",
        "   - ANOVA (Analysis of Variance) to compare variances across different groups.\n",
        "   - Regression analysis to assess the overall significance of the model.\n",
        "\n",
        "7. Critical Values: The critical values of the F-distribution can be found in F-distribution tables or calculated using statistical software, depending on the chosen significance level and the degrees of freedom.\n",
        "\n",
        "In summary, the F-distribution is essential for statistical inference, particularly when comparing variances and conducting hypothesis tests. Its properties, such as being skewed, having defined degrees of freedom, and specific mean and variance formulas, make it a unique and important distribution in statistics."
      ],
      "metadata": {
        "id": "-tYT2hlR_JVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## QUES 2)  In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "ebXWGMoI_UYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is primarily used in several types of statistical tests, most notably in the following:\n",
        "\n",
        "1. ANOVA (Analysis of Variance):\n",
        "   - ANOVA tests are used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others. The F-distribution is appropriate here because it helps assess the ratio of variances between the groups. If the group means are equal, the variances should be similar, leading to a smaller F-statistic. A larger F-statistic indicates that the group means are different.\n",
        "\n",
        "2. Regression Analysis:\n",
        "   - In multiple regression, the F-test is used to determine whether the overall regression model is a good fit for the data. Specifically, it tests the null hypothesis that all regression coefficients are equal to zero (no effect). The F-distribution is suitable because it compares the model variance explained by the predictors to the residual variance.\n",
        "\n",
        "3. Comparing Variances:\n",
        "   - The F-test can be used to compare the variances of two populations. This is particularly useful in assessing the assumption of homogeneity of variances, which is a critical assumption in many statistical tests, including t-tests and ANOVA.\n",
        "\n",
        "4. Mixed-Design ANOVA:\n",
        "   - This is a combination of repeated measures and independent measures ANOVA. The F-distribution is used to analyze the interaction effects between different factors.\n",
        "\n",
        "The appropriateness of the F-distribution in these tests stems from its properties, particularly its ability to model the ratio of variances. In many statistical procedures, the underlying assumptions include the normality of data and homogeneity of variances. The F-distribution provides a way to evaluate these assumptions and make inferences about population parameters based on sample data.\n",
        "\n",
        "Overall, the F-distribution is a critical component in the field of inferential statistics, allowing researchers to make valid conclusions about their data."
      ],
      "metadata": {
        "id": "a8TLgK1C_igk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## QUES 3) What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "# populations?"
      ],
      "metadata": {
        "id": "B4uFzQdW_pBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct an F-test for comparing the variances of two populations, there are several key assumptions that need to be met:\n",
        "\n",
        "1. Independence of Samples: The two samples being compared must be independent of each other. This means that the selection of one sample should not influence the selection of the other sample.\n",
        "\n",
        "2. Normality: The data in each of the populations should be approximately normally distributed. While the F-test is somewhat robust to violations of this assumption, significant deviations from normality can affect the validity of the test results.\n",
        "\n",
        "3. Homogeneity of Variances: The variances of the two populations should be equal (or at least similar). This assumption is crucial because the F-test specifically compares the ratio of the variances. If the variances are significantly different, the test results may not be reliable.\n",
        "\n",
        "4. Random Sampling: The samples should be randomly selected from their respective populations. This helps ensure that the samples are representative of the populations being studied.\n",
        "\n",
        "Meeting these assumptions is essential for the F-test to yield valid and reliable results. If any of these assumptions are violated, it may be necessary to use alternative statistical methods or transformations to analyze the data."
      ],
      "metadata": {
        "id": "TMxg9hw9_2AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## QUES 4) What is the purpose of ANOVA, and how does it differ from a t-test?"
      ],
      "metadata": {
        "id": "ETq6sdQO_4_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of ANOVA, which stands for Analysis of Variance, is to determine whether there are significant differences between the means of three or more groups. It helps researchers understand if at least one group mean is different from the others, which can indicate that different treatments or conditions have varying effects.\n",
        "\n",
        "The key difference between ANOVA and a t-test lies in the number of groups being compared. A t-test is used to compare the means of two groups only, while ANOVA can handle three or more groups simultaneously.\n",
        "\n",
        "Another important aspect is how they handle the analysis:\n",
        "\n",
        "- t-test: When conducting a t-test, you would typically perform multiple tests if you have more than two groups. This increases the risk of Type I error, which is the probability of incorrectly rejecting the null hypothesis.\n",
        "\n",
        "- ANOVA: ANOVA addresses this issue by providing a single test that evaluates all group means at once, thereby controlling for the overall Type I error rate. If ANOVA indicates significant differences, post-hoc tests can then be performed to determine which specific groups differ from each other.\n",
        "\n",
        "In summary, ANOVA is used for comparing three or more groups to see if there are significant differences in their means, while a t-test is suitable for comparing just two groups."
      ],
      "metadata": {
        "id": "f2bZeuE___yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## QUES 5) Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "# than two groups."
      ],
      "metadata": {
        "id": "eYoa3y7EAISY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups to avoid increasing the risk of Type I error.\n",
        "\n",
        "When you perform multiple t-tests, each test carries a chance of making a Type I error, which is incorrectly rejecting the null hypothesis. For example, if you conduct three t-tests, the overall probability of making at least one Type I error increases, making your results less reliable.\n",
        "\n",
        "A one-way ANOVA allows you to test for differences among three or more groups in a single analysis. It provides a way to assess whether there is a significant difference in means without inflating the error rate.\n",
        "\n",
        "If the ANOVA results indicate significant differences, you can then follow up with post-hoc tests to identify which specific groups differ from each other. This approach is more efficient and statistically sound than conducting multiple t-tests.\n",
        "\n",
        "In summary, use a one-way ANOVA when comparing more than two groups to control for Type I error and to streamline your analysis."
      ],
      "metadata": {
        "id": "bSJFiR0FAP5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## QUES 6) . Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "## How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "AbITOzcWAWKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In ANOVA, variance is partitioned into two main components: between-group variance and within-group variance. Understanding this partitioning is crucial for calculating the F-statistic, which helps determine if the group means are significantly different.\n",
        "\n",
        "1. Between-group variance: This measures the variability among the group means. It reflects how much the group means differ from the overall mean of all data points. If the group means are very different from each other, the between-group variance will be large. It's calculated by taking the sum of the squared differences between each group mean and the overall mean, multiplied by the number of observations in each group.\n",
        "\n",
        "2. Within-group variance: This measures the variability within each group. It reflects how much the individual observations within each group differ from their respective group mean. If the data points within each group are very similar to each other, the within-group variance will be small. It's calculated by taking the sum of the squared differences between each observation and its group mean across all groups.\n",
        "\n",
        "The partitioning of variance contributes to the calculation of the F-statistic as follows:\n",
        "\n",
        "- The F-statistic is calculated by dividing the mean square of the between-group variance by the mean square of the within-group variance.\n",
        "\n",
        "- The formula for the F-statistic is: F = (Mean Square Between) / (Mean Square Within).\n",
        "\n",
        "- The mean square values are obtained by dividing the sum of squares (the total variance) by their respective degrees of freedom.\n",
        "\n",
        "When the F-statistic is calculated, a larger value indicates that the between-group variance is greater relative to the within-group variance, suggesting that at least one group mean is significantly different from the others. If the F-statistic is statistically significant, it leads to the conclusion that not all group means are equal.\n",
        "\n",
        "In summary, partitioning variance into between-group and within-group components is essential in ANOVA, as it forms the basis for calculating the F-statistic, which helps determine if there are significant differences among group means."
      ],
      "metadata": {
        "id": "_ju_BaW8Ajjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## QUES 7) Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "# differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "i43xO1saAovP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classical (frequentist) approach to ANOVA and the Bayesian approach have several key differences in how they handle uncertainty, parameter estimation, and hypothesis testing.\n",
        "\n",
        "1. Handling Uncertainty:\n",
        "   - Frequentist Approach: In the frequentist framework, uncertainty is typically expressed through confidence intervals and p-values. It focuses on long-term frequencies of events under repeated sampling. For instance, a p-value indicates the probability of observing the data, or something more extreme, given that the null hypothesis is true.\n",
        "   - Bayesian Approach: The Bayesian approach incorporates prior beliefs about parameters and updates these beliefs with data to produce a posterior distribution. This method provides a direct probability statement about parameters, such as the probability that a parameter lies within a certain range.\n",
        "\n",
        "2. Parameter Estimation:\n",
        "   - Frequentist Approach: The frequentist approach estimates parameters using point estimates, such as the sample mean, and relies on methods like maximum likelihood estimation. The focus is on estimating a single value without incorporating prior information.\n",
        "   - Bayesian Approach: In contrast, the Bayesian approach provides a full posterior distribution for parameters, which reflects both the prior information and the data. This allows for a richer understanding of the uncertainty around parameter estimates, as you can derive credible intervals from the posterior distribution.\n",
        "\n",
        "3. Hypothesis Testing:\n",
        "   - Frequentist Approach: Hypothesis testing in the frequentist framework typically involves setting a null hypothesis and an alternative hypothesis, calculating a test statistic, and determining whether to reject the null hypothesis based on a p-value. The decision is often binary (reject or fail to reject).\n",
        "   - Bayesian Approach: The Bayesian approach evaluates hypotheses by calculating the posterior probabilities of the hypotheses themselves. Instead of a binary decision, it allows for a more nuanced understanding of how likely each hypothesis is given the data. Bayesian hypothesis testing can involve Bayes factors, which compare the evidence for different hypotheses.\n",
        "\n",
        "In summary, the classical approach to ANOVA focuses on long-term frequencies and point estimates, while the Bayesian approach incorporates prior beliefs and provides a full distribution of parameter estimates. Bayesian methods offer a more flexible framework for handling uncertainty and hypothesis testing, allowing for a richer interpretation of results."
      ],
      "metadata": {
        "id": "iM66S4l7Awtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## QUES 8)  Question: You have two sets of data representing the incomes of two different professions1\n",
        "# V Profession A: [48, 52, 55, 60, 62'\n",
        "# V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "# incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "# Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "# Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Data for Profession A and B\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate variances\n",
        "var_a = np.var(profession_a, ddof=1)\n",
        "var_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# Calculate F-statistic\n",
        "f_statistic = var_a / var_b\n",
        "\n",
        "# Degrees of freedom\n",
        "df_a = len(profession_a) - 1\n",
        "df_b = len(profession_b) - 1\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = 1 - f.cdf(f_statistic, df_a, df_b)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxTsa3PtA33W",
        "outputId": "ae061c26-f784-457b-cdb4-bb3a8d86c9f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "P-value: 0.24652429950266952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what the code does:\n",
        "\n",
        "Calculates the variances of the two professions' incomes.\n",
        "\n",
        "Computes the F-statistic by dividing the variance of Profession A by the variance of Profession B.\n",
        "\n",
        "Determines the degrees of freedom for both professions.\n",
        "\n",
        "Calculates the p-value using the cumulative distribution function (CDF) of the F-distribution.\n",
        "\n",
        "Interpretation:\n",
        "F-statistic: If the F-statistic is close to 1, it suggests that the variances are similar.\n",
        "\n",
        "P-value: A low p-value (typically less than 0.05) indicates that the variances are significantly different.\n",
        "\n",
        "Run the code to get the F-statistic and p-value, and then you can interpret the results based on the values obtained. If the p-value is less than 0.05, you can conclude that the variances of the two professions' incomes are significantly different. If the p-value is greater than 0.05, you fail to reject the null hypothesis, meaning the variances are not significantly different."
      ],
      "metadata": {
        "id": "KTrhgS3vBzUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## QUES 9) Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "# average heights between three different regions with the following data1\n",
        "# V Region A: [160, 162, 165, 158, 164'\n",
        "# V Region B: [172, 175, 170, 168, 174'\n",
        "# V Region C: [180, 182, 179, 185, 183'\n",
        "# V Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "# V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Data for Region A, B, and C\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGwRHOgyBUY3",
        "outputId": "42e5e468-9b6c-42db-b42f-ee760193e080"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "P-value: 2.870664187937026e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what the code does:\n",
        "\n",
        "Imports the necessary library: scipy.stats for statistical functions.\n",
        "\n",
        "Defines the data for the three regions.\n",
        "\n",
        "Performs the one-way ANOVA using stats.f_oneway().\n",
        "\n",
        "Prints the F-statistic and p-value.\n",
        "\n",
        "Interpretation:\n",
        "F-statistic: A higher F-statistic indicates a greater variance between the group means relative to the variance within the groups.\n",
        "\n",
        "P-value: A low p-value (typically less than 0.05) suggests that there are significant differences between the group means.\n",
        "\n",
        "Run the code to get the F-statistic and p-value, and then you can interpret the results based on the values obtained. If the p-value is less than 0.05, you can conclude that there are statistically significant differences in average heights between the three regions. If the p-value is greater than 0.05, you fail to reject the null hypothesis, meaning there are no significant differences in average heights between the regions."
      ],
      "metadata": {
        "id": "ujIkQ4m1CV4x"
      }
    }
  ]
}